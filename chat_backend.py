# -*- encoding: utf-8 -*-
'''
@File    :   chat_backend.py
@Time    :   2025/04/11
@Author  :   Yansong Du 
@Contact :   dys24@mails.tsinghua.edu.cn
'''

from fastapi import FastAPI
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel, LoraConfig, TaskType
from fastapi.middleware.cors import CORSMiddleware
import torch

app = FastAPI()

# è·¨åŸŸæ”¯æŒï¼ˆå…è®¸ç½‘é¡µè®¿é—®ï¼‰
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# é¦–é¡µæç¤º
@app.get("/")
def root():
    return {"message": "ğŸŒ Qwen + LoRA API å·²å°±ç»ªï¼Œä½¿ç”¨ POST /chat æ¥å£å‘é€ {'question': '...'}ã€‚"}

# è¾“å…¥æ•°æ®ç»“æ„
class Query(BaseModel):
    question: str

# æ¨¡å‹é…ç½®
model_path = "./qwen/Qwen2___5-1___5B-Instruct"
lora_path = "output/Qwen2.5-1.5b/checkpoint-24"

try:
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    # âœ… å›ºå®šåŠ è½½åˆ° cuda:0ï¼Œé¿å… device_map="auto" åˆ†å¡å¯¼è‡´é”™è¯¯
    base_model = AutoModelForCausalLM.from_pretrained(
        model_path,
        device_map={"": 0},
        torch_dtype=torch.bfloat16
    )

    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        inference_mode=True, r=64, lora_alpha=16, lora_dropout=0.1
    )

    model = PeftModel.from_pretrained(base_model, model_id=lora_path, config=lora_config)
    model = model.to("cuda:0")
    model.eval()
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼ŒæœåŠ¡å·²å°±ç»ªã€‚")

except Exception as e:
    print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{e}")
    raise e

# å¯¹è¯æ¥å£
@app.post("/chat")
def chat(query: Query):
    if not query.question.strip():
        return {"response": "âš ï¸ è¯·è¾“å…¥ä¸€ä¸ªæœ‰æ•ˆçš„é—®é¢˜ã€‚"}

    try:
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ— çº¿å…‰é€šä¿¡é¢†åŸŸçš„ä¸“å®¶ï¼Œè¯·æ ¹æ®ç»™å‡ºçš„æ— çº¿å…‰é€šä¿¡ä¸“ä¸šé¢†åŸŸå†…çš„é—®é¢˜å›ç­”ç›¸å…³çš„å†…å®¹"},
            {"role": "user", "content": query.question}
        ]
        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

        # âœ… ä¿ç•™ BatchEncoding ç±»å‹ï¼ŒåŒæ—¶è¿ç§»è¾“å…¥å¼ é‡åˆ° cuda:0
        inputs = tokenizer([text], return_tensors="pt", padding=True)
        inputs = inputs.to("cuda:0")
        inputs["attention_mask"] = (inputs["input_ids"] != tokenizer.pad_token_id).long()

        outputs = model.generate(
            input_ids=inputs.input_ids,
            attention_mask=inputs.attention_mask,
            max_new_tokens=512
        )
        outputs = outputs[:, inputs.input_ids.shape[1]:]
        response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]
        return {"response": response}

    except Exception as e:
        return {"response": f"âŒ æ¨¡å‹æ¨ç†å¤±è´¥ï¼š{str(e)}"}

if __name__ == "__main__":

   import uvicorn
   uvicorn.run(app, host="0.0.0.0", port=8000)